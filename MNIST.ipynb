{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i699BCsdsfx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import levy_stable\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.01\n",
        "CLIP = 5\n",
        "NOISE_INIT = 0.005\n",
        "NOISE_DECAY = 0.55\n",
        "EPOCHS = 10\n",
        "\n",
        "# =====================================\n",
        "# 1. Optimizer Definitions\n",
        "# =====================================\n",
        "\n",
        "class AnnealisingOptimiser(Optimizer):\n",
        "    \"\"\"\n",
        "    A heavy-tailed optimizer that applies a fixed, non-adaptive update.\n",
        "    The parameter alpha is updated as: alpha = 2 - exp(-k * num_steps)\n",
        "    Noise variance decays per step.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 params,\n",
        "                 lr=1e-3,\n",
        "                 alpha=1.2,\n",
        "                 rho=0.05,\n",
        "                 decay=0.95,\n",
        "                 noise_init=NOISE_INIT,\n",
        "                 noise_decay=NOISE_DECAY,\n",
        "                 weight_decay=0,\n",
        "                 k=1/5,\n",
        "                 **kwargs):\n",
        "        defaults = dict(lr=lr, alpha=alpha, rho=rho, weight_decay=weight_decay)\n",
        "        super(AnnealisingOptimiser, self).__init__(params, defaults)\n",
        "        self.decay = decay\n",
        "        self.noise_init = noise_init\n",
        "        self.noise_decay = noise_decay\n",
        "        self.beta = 0  # symmetric noise\n",
        "        self.num_steps = 1\n",
        "        self.current_sigma = noise_init\n",
        "        self.noise_samples = []\n",
        "        self.epoch = 0\n",
        "        self.k = k\n",
        "\n",
        "    def update_epoch(self):\n",
        "        self.epoch += 1\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        if closure is None:\n",
        "            raise ValueError(\"Requires closure for loss computation\")\n",
        "        closure = torch.enable_grad()(closure)\n",
        "        loss = closure()\n",
        "\n",
        "        # Update alpha and noise scale\n",
        "        self.alpha = 2 - math.exp(-self.k * self.num_steps)\n",
        "        self.current_sigma = np.sqrt(self.noise_init) / ((1 + self.num_steps) ** (self.noise_decay/2))\n",
        "\n",
        "        total_elems = sum(p.numel() for group in self.param_groups for p in group['params'])\n",
        "        big_noise = levy_stable.rvs(self.alpha, self.beta, scale=self.current_sigma, size=total_elems)\n",
        "        np.clip(big_noise, -CLIP, CLIP, out=big_noise)\n",
        "        device = self.param_groups[0]['params'][0].device\n",
        "        big_noise = torch.from_numpy(big_noise).float().to(device=device)\n",
        "        self.noise_samples.append(big_noise.detach().cpu().numpy())\n",
        "\n",
        "        idx_start = 0\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p.grad.data.add_(p.data, alpha=group['weight_decay'])\n",
        "                elem_count = p.numel()\n",
        "                noise_slice = big_noise[idx_start:idx_start + elem_count].view_as(p.data)\n",
        "                idx_start += elem_count\n",
        "                noise_coeff = (lr ** (1 / self.alpha)) * noise_slice\n",
        "                p.data.add_(p.grad.data, alpha=-lr)\n",
        "                p.data.add_(noise_coeff)\n",
        "        self.num_steps += 1\n",
        "        return loss.item()\n",
        "\n",
        "class ImprovedGaussianOptimizer(Optimizer):\n",
        "    \"\"\"\n",
        "    An optimizer that applies Gaussian noise (mean=0) to the gradient update.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 params,\n",
        "                 lr=1e-3,\n",
        "                 noise_init=0.01,\n",
        "                 noise_decay=NOISE_DECAY,\n",
        "                 weight_decay=0,\n",
        "                 **kwargs):\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        super(ImprovedGaussianOptimizer, self).__init__(params, defaults)\n",
        "        self.noise_init = noise_init\n",
        "        self.noise_decay = noise_decay\n",
        "        self.num_steps = 1\n",
        "        self.epoch = 0\n",
        "        self.current_sigma = noise_init\n",
        "\n",
        "    def update_epoch(self):\n",
        "        self.epoch += 1\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        if closure is None:\n",
        "            raise ValueError(\"Requires closure for loss computation\")\n",
        "        closure = torch.enable_grad()(closure)\n",
        "        loss = closure()\n",
        "        self.current_sigma = self.noise_init / ((1 + self.num_steps) ** self.noise_decay)\n",
        "        total_elems = sum(p.numel() for group in self.param_groups for p in group['params'])\n",
        "        device = self.param_groups[0]['params'][0].device\n",
        "        noise = torch.randn(total_elems, device=device) * self.current_sigma\n",
        "        idx_start = 0\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                elem_count = p.numel()\n",
        "                noise_slice = noise[idx_start:idx_start + elem_count].view_as(p.data)\n",
        "                idx_start += elem_count\n",
        "                p.data.add_(p.grad.data + noise_slice, alpha=-lr)\n",
        "        self.num_steps += 1\n",
        "        return loss.item()\n",
        "\n",
        "class AdaptiveAnnealisingOptimizer(Optimizer):\n",
        "    \"\"\"\n",
        "    An optimizer that adapts its alpha parameter using a logistic mapping from the EMA of the Hessian trace (sharpness)\n",
        "    to the interval [alpha_min, alpha_max].\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 params,\n",
        "                 lr=1e-3,\n",
        "                 momentum=0.0,\n",
        "                 alpha=1.0,\n",
        "                 alpha_min=1.0,\n",
        "                 alpha_max=2.0,\n",
        "                 window_size=20,\n",
        "                 noise_init=0.001,\n",
        "                 noise_decay=0.55,\n",
        "                 weight_decay=0,\n",
        "                 num_hutchinson_samples=1,\n",
        "                 logistic_scale=0.5,\n",
        "                 logistic_center=3.0):\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        super(AdaptiveAnnealisingOptimizer, self).__init__(params, defaults)\n",
        "        self.alpha_min = alpha_min\n",
        "        self.alpha_max = alpha_max\n",
        "        self.window_size = window_size\n",
        "        self.noise_init = noise_init\n",
        "        self.noise_decay = noise_decay\n",
        "        self.num_hutchinson_samples = num_hutchinson_samples\n",
        "        self.logistic_scale = logistic_scale\n",
        "        self.logistic_center = logistic_center\n",
        "        self.alpha = alpha\n",
        "        self.beta = 0\n",
        "        self.num_steps = 1\n",
        "        self.current_sigma = noise_init\n",
        "        self.noise_samples = []\n",
        "        self.sharpness_list = []\n",
        "        self.ema_list = []\n",
        "        self.alpha_list = []\n",
        "        self.epoch = 0\n",
        "        self.ema_sharpness = None\n",
        "\n",
        "    def update_epoch(self):\n",
        "        self.epoch += 1\n",
        "\n",
        "    def compute_hessian_trace(self, loss, params):\n",
        "        trace_est = 0.0\n",
        "        for _ in range(self.num_hutchinson_samples):\n",
        "            r = [torch.randint(0, 2, p.shape, device=p.device).float()*2 - 1 for p in params]\n",
        "            grads = torch.autograd.grad(loss, params, create_graph=True, retain_graph=True)\n",
        "            dot = sum((g * r_i).sum() for g, r_i in zip(grads, r))\n",
        "            hv = torch.autograd.grad(dot, params, retain_graph=True)\n",
        "            trace_est += sum((r_i * hv_i).sum() for r_i, hv_i in zip(r, hv))\n",
        "        trace_est /= self.num_hutchinson_samples\n",
        "        return trace_est\n",
        "\n",
        "    def adapt_alpha(self, sharpness):\n",
        "        alpha_ema = 0.1\n",
        "        alpha_smoothing = 0.1\n",
        "        if self.ema_sharpness is None:\n",
        "            self.ema_sharpness = sharpness\n",
        "            self.ema_list.append(self.ema_sharpness)\n",
        "            self.alpha = self.alpha_min\n",
        "            self.alpha_list.append(self.alpha)\n",
        "            return\n",
        "        self.ema_sharpness = (1 - alpha_ema)*self.ema_sharpness + alpha_ema*sharpness\n",
        "        self.ema_list.append(self.ema_sharpness)\n",
        "        if len(self.ema_list) < self.window_size:\n",
        "            mean_recent = self.ema_sharpness\n",
        "        else:\n",
        "            mean_recent = np.mean(self.ema_list[-self.window_size:])\n",
        "        A = self.alpha_min\n",
        "        B = self.alpha_max\n",
        "        k = self.logistic_scale\n",
        "        c = self.logistic_center\n",
        "        logistic_val = 1.0 / (1.0 + math.exp(-k*(mean_recent-c)))\n",
        "        new_alpha_raw = A + (B-A)*logistic_val\n",
        "        self.alpha = self.alpha + alpha_smoothing*(new_alpha_raw - self.alpha)\n",
        "        self.alpha_list.append(self.alpha)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        if closure is None:\n",
        "            raise ValueError(\"AdaptiveAnnealisingOptimizer requires a closure.\")\n",
        "        with torch.enable_grad():\n",
        "            loss = closure()\n",
        "            params = [p for group in self.param_groups for p in group['params'] if p.requires_grad]\n",
        "            hessian_trace = self.compute_hessian_trace(loss, params)\n",
        "            sharpness_value = torch.log1p(torch.abs(hessian_trace)).item()\n",
        "            self.sharpness_list.append(sharpness_value)\n",
        "            self.adapt_alpha(sharpness_value)\n",
        "            loss.backward()\n",
        "        with torch.no_grad():\n",
        "            self.current_sigma = np.sqrt(self.noise_init) / ((1 + self.num_steps)**(self.noise_decay/2))\n",
        "            total_elems = sum(p.numel() for group in self.param_groups for p in group['params'])\n",
        "            big_noise = levy_stable.rvs(self.alpha, self.beta, scale=self.current_sigma, size=total_elems)\n",
        "            np.clip(big_noise, -CLIP, CLIP, out=big_noise)\n",
        "            self.noise_samples.append(big_noise)\n",
        "            device = self.param_groups[0]['params'][0].device\n",
        "            big_noise = torch.from_numpy(big_noise).float().to(device=device)\n",
        "            idx_start = 0\n",
        "            for group in self.param_groups:\n",
        "                lr = group['lr']\n",
        "                momentum = group['momentum']\n",
        "                for p in group['params']:\n",
        "                    if p.grad is None:\n",
        "                        continue\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p.grad.data.add_(p.data, alpha=group['weight_decay'])\n",
        "                    elem_count = p.numel()\n",
        "                    noise_slice = big_noise[idx_start:idx_start+elem_count].view_as(p.grad.data)\n",
        "                    idx_start += elem_count\n",
        "                    noise_coeff = (lr**(1/self.alpha)) * noise_slice\n",
        "                    if momentum != 0:\n",
        "                        state = self.state[p]\n",
        "                        if 'momentum_buffer' not in state:\n",
        "                            buf = state['momentum_buffer'] = torch.zeros_like(p.data)\n",
        "                        else:\n",
        "                            buf = state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(p.grad.data + noise_coeff)\n",
        "                        p.data.add_(buf, alpha=-lr)\n",
        "                    else:\n",
        "                        p.data.add_(p.grad.data, alpha=-lr)\n",
        "                        p.data.add_(noise_coeff)\n",
        "            self.num_steps += 1\n",
        "        return loss.item()\n",
        "\n",
        "class SAM(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements Sharpness-Aware Minimization (SAM).\n",
        "    This optimizer wraps a base optimizer (e.g. SGD) and perturbs the weights along the ascent direction.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
        "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.rho = rho\n",
        "        self.adaptive = adaptive\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device\n",
        "        norm = torch.norm(torch.stack([\n",
        "            ((torch.abs(p) if self.adaptive else 1.0)*p.grad).norm(p=2).to(shared_device)\n",
        "            for group in self.param_groups for p in group[\"params\"] if p.grad is not None\n",
        "        ]), p=2)\n",
        "        return norm\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=True):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = self.rho/(grad_norm+1e-12)\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                e_w = (p.abs() if self.adaptive else 1.0)*p.grad*scale\n",
        "                p.add_(e_w)\n",
        "                self.state[p][\"e_w\"] = e_w\n",
        "        if zero_grad:\n",
        "            self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=True):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                p.sub_(self.state[p][\"e_w\"])\n",
        "        self.base_optimizer.step()\n",
        "        if zero_grad:\n",
        "            self.zero_grad()\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        if closure is None:\n",
        "            raise ValueError(\"SAM requires a closure.\")\n",
        "        closure = torch.enable_grad()(closure)\n",
        "        loss = closure()\n",
        "        loss.backward()\n",
        "        self.first_step(zero_grad=True)\n",
        "        loss_perturbed = closure()\n",
        "        loss_perturbed.backward()\n",
        "        self.second_step(zero_grad=True)\n",
        "        return loss.item()\n",
        "\n",
        "# =====================================\n",
        "# 2. Simple MLP Model Definition (MNIST)\n",
        "# =====================================\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList(\n",
        "            [nn.Linear(28*28, 50)] + [nn.Linear(50, 50) for _ in range(3)]\n",
        "        )\n",
        "        self.output_layer = nn.Linear(50, 10)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.constant_(m.weight, 0)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        for layer in self.hidden_layers:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# =====================================\n",
        "# 3. Data Loading (MNIST)\n",
        "# =====================================\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# =====================================\n",
        "# 4. Train & Test Utilities\n",
        "# =====================================\n",
        "\n",
        "def train_sgd(model, loader, optimizer, epoch, losses):\n",
        "    model.train()\n",
        "    running_loss, total = 0.0, 0\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        total += 1\n",
        "    losses.append(running_loss/total)\n",
        "\n",
        "def train_noise(model, loader, optimizer, epoch, losses, noise_list):\n",
        "    model.train()\n",
        "    running_loss, total = 0.0, 0\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        loss_val = optimizer.step(closure=closure)\n",
        "        running_loss += loss_val\n",
        "        total += 1\n",
        "    losses.append(running_loss/total)\n",
        "    noise_list.append(optimizer.current_sigma)\n",
        "\n",
        "def train_adaptive(model, loader, optimizer, epoch, losses, alpha_list, sharpness_list, noise_list):\n",
        "    model.train()\n",
        "    running_loss, total = 0.0, 0\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            return F.cross_entropy(output, target)\n",
        "        loss_val = optimizer.step(closure=closure)\n",
        "        running_loss += loss_val\n",
        "        total += 1\n",
        "    losses.append(running_loss/total)\n",
        "    if optimizer.sharpness_list:\n",
        "        sharpness_list.append(optimizer.sharpness_list[-1])\n",
        "    else:\n",
        "        sharpness_list.append(0.0)\n",
        "    if optimizer.alpha_list:\n",
        "        alpha_list.append(optimizer.alpha_list[-1])\n",
        "    else:\n",
        "        alpha_list.append(optimizer.alpha)\n",
        "    noise_list.append(optimizer.current_sigma)\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    total_loss, correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target, reduction='sum')\n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    return total_loss/len(loader.dataset), 100.0*correct/len(loader.dataset)\n",
        "\n",
        "def train_sam(model, loader, optimizer, epoch, losses, sam_rho_list):\n",
        "    model.train()\n",
        "    running_loss, total = 0.0, 0\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            return F.cross_entropy(output, target)\n",
        "        loss_val = optimizer.step(closure=closure)\n",
        "        running_loss += loss_val\n",
        "        total += 1\n",
        "        sam_rho_list.append(optimizer.rho)\n",
        "    losses.append(running_loss/total)\n",
        "\n",
        "# =====================================\n",
        "# 5. Main Training Loop & Metric Collection\n",
        "# =====================================\n",
        "\n",
        "# Save initial state for reinitialization\n",
        "model_ref = SimpleMLP().to(device)\n",
        "initial_state = copy.deepcopy(model_ref.state_dict())\n",
        "\n",
        "# ----- Instantiate Models & Optimizers -----\n",
        "# Annealising Optimizer with k = 1/1000\n",
        "model_k1000 = SimpleMLP().to(device)\n",
        "model_k1000.load_state_dict(initial_state)\n",
        "optimizer_k1000 = AnnealisingOptimiser(\n",
        "    model_k1000.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    alpha=1.7,\n",
        "    rho=0.05,\n",
        "    decay=0.95,\n",
        "    noise_init=NOISE_INIT,\n",
        "    noise_decay=NOISE_DECAY,\n",
        "    k=1/1000,\n",
        "    weight_decay=0\n",
        ")\n",
        "\n",
        "# SGD\n",
        "model_sgd = SimpleMLP().to(device)\n",
        "model_sgd.load_state_dict(initial_state)\n",
        "optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=LEARNING_RATE, momentum=0.0, weight_decay=0)\n",
        "\n",
        "# Gaussian\n",
        "model_gaussian = SimpleMLP().to(device)\n",
        "model_gaussian.load_state_dict(initial_state)\n",
        "optimizer_gaussian = ImprovedGaussianOptimizer(\n",
        "    model_gaussian.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    noise_init=NOISE_INIT,\n",
        "    noise_decay=NOISE_DECAY,\n",
        "    weight_decay=0\n",
        ")\n",
        "\n",
        "# Annealising Optimizer with k = 1/5\n",
        "model_k5 = SimpleMLP().to(device)\n",
        "model_k5.load_state_dict(initial_state)\n",
        "optimizer_k5 = AnnealisingOptimiser(\n",
        "    model_k5.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    alpha=1.7,\n",
        "    rho=0.05,\n",
        "    decay=0.95,\n",
        "    noise_init=NOISE_INIT,\n",
        "    noise_decay=NOISE_DECAY,\n",
        "    k=1/5,\n",
        "    weight_decay=0\n",
        ")\n",
        "\n",
        "# HT-Hutch Optimizer with k = 1/10 (using AnnealisingOptimiser here)\n",
        "model_k10 = SimpleMLP().to(device)\n",
        "model_k10.load_state_dict(initial_state)\n",
        "optimizer_k10 = AnnealisingOptimiser(\n",
        "    model_k10.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    alpha=1.7,\n",
        "    rho=0.05,\n",
        "    decay=0.95,\n",
        "    noise_init=NOISE_INIT,\n",
        "    noise_decay=NOISE_DECAY,\n",
        "    k=1/10,\n",
        "    weight_decay=0\n",
        ")\n",
        "\n",
        "# HT-Power Optimizer with k = 1/50 (using AnnealisingOptimiser here)\n",
        "model_k50 = SimpleMLP().to(device)\n",
        "model_k50.load_state_dict(initial_state)\n",
        "optimizer_k50 = AnnealisingOptimiser(\n",
        "    model_k50.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    alpha=1.7,\n",
        "    rho=0.05,\n",
        "    decay=0.95,\n",
        "    noise_init=NOISE_INIT,\n",
        "    noise_decay=NOISE_DECAY,\n",
        "    k=1/50,\n",
        "    weight_decay=0\n",
        ")\n",
        "\n",
        "# AdaptiveAnnealisingOptimizer\n",
        "model_adaptive = SimpleMLP().to(device)\n",
        "model_adaptive.load_state_dict(initial_state)\n",
        "optimizer_adaptive = AdaptiveAnnealisingOptimizer(\n",
        "    model_adaptive.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    momentum=0.0,\n",
        "    alpha_min=1.0,\n",
        "    alpha_max=2.0,\n",
        "    window_size=50,\n",
        "    noise_init=NOISE_INIT,\n",
        "    noise_decay=NOISE_DECAY,\n",
        "    weight_decay=0,\n",
        "    num_hutchinson_samples=1\n",
        ")\n",
        "\n",
        "# SAM (using AnnealisingOptimiser with k = 1/100)\n",
        "model_k100 = SimpleMLP().to(device)\n",
        "model_k100.load_state_dict(initial_state)\n",
        "optimizer_k100 = AnnealisingOptimiser(\n",
        "    model_k100.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    alpha=1.7,\n",
        "    rho=0.05,\n",
        "    decay=0.95,\n",
        "    noise_init=NOISE_INIT,\n",
        "    noise_decay=NOISE_DECAY,\n",
        "    k=1/100,\n",
        "    weight_decay=0\n",
        ")\n",
        "\n",
        "# ----- Initialize Metric Lists -----\n",
        "sgd_train_losses, sgd_test_losses, sgd_test_errors, sgd_train_acc = [], [], [], []\n",
        "gauss_train_losses, gauss_test_losses, gauss_test_errors, gauss_train_acc, gauss_noise = [], [], [], [], []\n",
        "k1000_train_losses, k1000_test_losses, k1000_test_errors, k1000_train_acc = [], [], [], []\n",
        "k5_train_losses, k5_test_losses, k5_test_errors, k5_train_acc, k5_noise = [], [], [], [], []\n",
        "k10_train_losses, k10_test_losses, k10_test_errors, k10_train_acc = [], [], [], []\n",
        "k50_train_losses, k50_test_losses, k50_test_errors, k50_train_acc = [], [], [], []\n",
        "adaptive_train_losses, adaptive_test_losses, adaptive_test_errors, adaptive_train_acc = [], [], [], []\n",
        "adaptive_alpha, adaptive_sharpness, adaptive_noise = [], [], []\n",
        "k100_train_losses, k100_test_losses, k100_test_errors, k100_train_acc, k100_noise = [], [], [], [], []\n",
        "k100_rho_list = []\n",
        "\n",
        "# ----- Main Training Loop -----\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    # SGD\n",
        "    train_sgd(model_sgd, train_loader, optimizer_sgd, ep, sgd_train_losses)\n",
        "    loss_sgd, acc_sgd = evaluate_model(model_sgd, train_loader)\n",
        "    sgd_train_acc.append(acc_sgd)\n",
        "    loss_sgd_test, acc_sgd_test = evaluate_model(model_sgd, test_loader)\n",
        "    sgd_test_losses.append(loss_sgd_test)\n",
        "    sgd_test_errors.append(1.0 - (acc_sgd_test/100.0))\n",
        "    print(f\"[SGD] Epoch {ep} | Train Loss: {loss_sgd:.4f} | Train Acc: {acc_sgd:.2f}% | Test Loss: {loss_sgd_test:.4f} | Test Acc: {acc_sgd_test:.2f}%\")\n",
        "\n",
        "    # Gaussian\n",
        "    train_noise(model_gaussian, train_loader, optimizer_gaussian, ep, gauss_train_losses, gauss_noise)\n",
        "    loss_gauss, acc_gauss = evaluate_model(model_gaussian, train_loader)\n",
        "    gauss_train_acc.append(acc_gauss)\n",
        "    loss_gauss_test, acc_gauss_test = evaluate_model(model_gaussian, test_loader)\n",
        "    gauss_test_losses.append(loss_gauss_test)\n",
        "    gauss_test_errors.append(1.0 - (acc_gauss_test/100.0))\n",
        "    print(f\"[Gaussian] Epoch {ep} | Train Loss: {loss_gauss:.4f} | Train Acc: {acc_gauss:.2f}% | Test Loss: {loss_gauss_test:.4f} | Test Acc: {acc_gauss_test:.2f}%\")\n",
        "\n",
        "    # k = 1/1000 (HT)\n",
        "    train_noise(model_k1000, train_loader, optimizer_k1000, ep, k1000_train_losses, [])\n",
        "    loss_k1000, acc_k1000 = evaluate_model(model_k1000, train_loader)\n",
        "    k1000_train_acc.append(acc_k1000)\n",
        "    loss_k1000_test, acc_k1000_test = evaluate_model(model_k1000, test_loader)\n",
        "    k1000_test_losses.append(loss_k1000_test)\n",
        "    k1000_test_errors.append(1.0 - (acc_k1000_test/100.0))\n",
        "    print(f\"[k=1/1000] Epoch {ep} | Train Loss: {loss_k1000:.4f} | Train Acc: {acc_k1000:.2f}% | Test Loss: {loss_k1000_test:.4f} | Test Acc: {acc_k1000_test:.2f}%\")\n",
        "\n",
        "    # k = 1/5 (Annealising)\n",
        "    train_noise(model_k5, train_loader, optimizer_k5, ep, k5_train_losses, k5_noise)\n",
        "    loss_k5, acc_k5 = evaluate_model(model_k5, train_loader)\n",
        "    k5_train_acc.append(acc_k5)\n",
        "    loss_k5_test, acc_k5_test = evaluate_model(model_k5, test_loader)\n",
        "    k5_test_losses.append(loss_k5_test)\n",
        "    k5_test_errors.append(1.0 - (acc_k5_test/100.0))\n",
        "    print(f\"[k=1/5] Epoch {ep} | Train Loss: {loss_k5:.4f} | Train Acc: {acc_k5:.2f}% | Test Loss: {loss_k5_test:.4f} | Test Acc: {acc_k5_test:.2f}%\")\n",
        "\n",
        "    # k = 1/10 (HT-Hutch)\n",
        "    train_noise(model_k10, train_loader, optimizer_k10, ep, k10_train_losses, [])\n",
        "    loss_k10, acc_k10 = evaluate_model(model_k10, train_loader)\n",
        "    k10_train_acc.append(acc_k10)\n",
        "    loss_k10_test, acc_k10_test = evaluate_model(model_k10, test_loader)\n",
        "    k10_test_losses.append(loss_k10_test)\n",
        "    k10_test_errors.append(1.0 - (acc_k10_test/100.0))\n",
        "    print(f\"[k=1/10] Epoch {ep} | Train Loss: {loss_k10:.4f} | Train Acc: {acc_k10:.2f}% | Test Loss: {loss_k10_test:.4f} | Test Acc: {acc_k10_test:.2f}%\")\n",
        "\n",
        "    # k = 1/50 (HT-Power)\n",
        "    train_noise(model_k50, train_loader, optimizer_k50, ep, k50_train_losses, [])\n",
        "    loss_k50, acc_k50 = evaluate_model(model_k50, train_loader)\n",
        "    k50_train_acc.append(acc_k50)\n",
        "    loss_k50_test, acc_k50_test = evaluate_model(model_k50, test_loader)\n",
        "    k50_test_losses.append(loss_k50_test)\n",
        "    k50_test_errors.append(1.0 - (acc_k50_test/100.0))\n",
        "    print(f\"[k=1/50] Epoch {ep} | Train Loss: {loss_k50:.4f} | Train Acc: {acc_k50:.2f}% | Test Loss: {loss_k50_test:.4f} | Test Acc: {acc_k50_test:.2f}%\")\n",
        "\n",
        "    # Adaptive\n",
        "    train_adaptive(model_adaptive, train_loader, optimizer_adaptive, ep, adaptive_train_losses, adaptive_alpha, adaptive_sharpness, adaptive_noise)\n",
        "    loss_adaptive, acc_adaptive = evaluate_model(model_adaptive, train_loader)\n",
        "    adaptive_train_acc.append(acc_adaptive)\n",
        "    loss_adaptive_test, acc_adaptive_test = evaluate_model(model_adaptive, test_loader)\n",
        "    adaptive_test_losses.append(loss_adaptive_test)\n",
        "    adaptive_test_errors.append(1.0 - (acc_adaptive_test/100.0))\n",
        "    print(f\"[Adaptive] Epoch {ep} | Train Loss: {loss_adaptive:.4f} | Train Acc: {acc_adaptive:.2f}% | Test Loss: {loss_adaptive_test:.4f} | Test Acc: {acc_adaptive_test:.2f}%\")\n",
        "\n",
        "    # k = 1/100 (SAM)\n",
        "    train_noise(model_k100, train_loader, optimizer_k100, ep, k100_train_losses, k100_noise)\n",
        "    loss_k100, acc_k100 = evaluate_model(model_k100, train_loader)\n",
        "    k100_train_acc.append(acc_k100)\n",
        "    loss_k100_test, acc_k100_test = evaluate_model(model_k100, test_loader)\n",
        "    k100_test_losses.append(loss_k100_test)\n",
        "    k100_test_errors.append(1.0 - (acc_k100_test/100.0))\n",
        "    k100_rho_list.append(optimizer_k100.rho)\n",
        "    print(f\"[k=1/100] Epoch {ep} | Train Loss: {loss_k100:.4f} | Train Acc: {acc_k100:.2f}% | Test Loss: {loss_k100_test:.4f} | Test Acc: {acc_k100_test:.2f}%\")\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "epochs_list = list(range(1, EPOCHS + 1))\n",
        "\n",
        "# =====================================\n",
        "# 6. Overlaid Plots for All Optimizers (MNIST)\n",
        "# =====================================\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs_list, sgd_train_losses, label=\"SGD\", linewidth=2, color='blue')\n",
        "plt.plot(epochs_list, gauss_train_losses, label=\"Gaussian\", linewidth=2, color='red')\n",
        "plt.plot(epochs_list, k1000_train_losses, label=\"k = 1/1000\", linewidth=2, color='black')\n",
        "plt.plot(epochs_list, k5_train_losses, label=\"k = 1/5\", linewidth=2, color='green')\n",
        "plt.plot(epochs_list, k10_train_losses, label=\"k = 1/10\", linewidth=2, color='purple')\n",
        "plt.plot(epochs_list, k50_train_losses, label=\"k = 1/50\", linewidth=2, color='orange')\n",
        "plt.plot(epochs_list, adaptive_train_losses, label=\"Adaptive\", linewidth=2, color='brown')\n",
        "plt.plot(epochs_list, k100_train_losses, label=\"k = 1/100\", linewidth=2, color='magenta')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss (Cross-Entropy)\")\n",
        "plt.title(\"Training Loss - All Optimizers (MNIST)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mnist_all_train_loss.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs_list, sgd_test_losses, label=\"SGD\", linewidth=2, color='blue')\n",
        "plt.plot(epochs_list, gauss_test_losses, label=\"Gaussian\", linewidth=2, color='red')\n",
        "plt.plot(epochs_list, k1000_test_losses, label=\"k = 1/1000\", linewidth=2, color='black')\n",
        "plt.plot(epochs_list, k5_test_losses, label=\"k = 1/5\", linewidth=2, color='green')\n",
        "plt.plot(epochs_list, k10_test_losses, label=\"k = 1/10\", linewidth=2, color='purple')\n",
        "plt.plot(epochs_list, k50_test_losses, label=\"k = 1/50\", linewidth=2, color='orange')\n",
        "plt.plot(epochs_list, adaptive_test_losses, label=\"Adaptive\", linewidth=2, color='brown')\n",
        "plt.plot(epochs_list, k100_test_losses, label=\"k = 1/100\", linewidth=2, color='magenta')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Test Loss\")\n",
        "plt.title(\"Test Loss - All Optimizers (MNIST)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mnist_all_test_loss.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "sgd_test_acc = [(1 - err)*100 for err in sgd_test_errors]\n",
        "gauss_test_acc = [(1 - err)*100 for err in gauss_test_errors]\n",
        "k1000_test_acc = [(1 - err)*100 for err in k1000_test_errors]\n",
        "k5_test_acc = [(1 - err)*100 for err in k5_test_errors]\n",
        "k10_test_acc = [(1 - err)*100 for err in k10_test_errors]\n",
        "k50_test_acc = [(1 - err)*100 for err in k50_test_errors]\n",
        "adaptive_test_acc = [(1 - err)*100 for err in adaptive_test_errors]\n",
        "k100_test_acc = [(1 - err)*100 for err in k100_test_errors]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs_list, sgd_test_acc, label=\"SGD\", linewidth=2, color='blue')\n",
        "plt.plot(epochs_list, gauss_test_acc, label=\"Gaussian\", linewidth=2, color='red')\n",
        "plt.plot(epochs_list, k1000_test_acc, label=\"k = 1/1000\", linewidth=2, color='black')\n",
        "plt.plot(epochs_list, k5_test_acc, label=\"k = 1/5\", linewidth=2, color='green')\n",
        "plt.plot(epochs_list, k10_test_acc, label=\"k = 1/10\", linewidth=2, color='purple')\n",
        "plt.plot(epochs_list, k50_test_acc, label=\"k = 1/50\", linewidth=2, color='orange')\n",
        "plt.plot(epochs_list, adaptive_test_acc, label=\"Adaptive\", linewidth=2, color='brown')\n",
        "plt.plot(epochs_list, k100_test_acc, label=\"k = 1/100\", linewidth=2, color='magenta')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Test Accuracy (%)\")\n",
        "plt.title(\"Test Accuracy - All Optimizers (MNIST)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mnist_all_test_accuracy.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs_list, sgd_train_acc, label=\"SGD\", linewidth=2, color='blue')\n",
        "plt.plot(epochs_list, gauss_train_acc, label=\"Gaussian\", linewidth=2, color='red')\n",
        "plt.plot(epochs_list, k5_train_acc, label=\"k = 1/5\", linewidth=2, color='green')\n",
        "plt.plot(epochs_list, k10_train_acc, label=\"k = 1/10\", linewidth=2, color='purple')\n",
        "plt.plot(epochs_list, k50_train_acc, label=\"k = 1/50\", linewidth=2, color='orange')\n",
        "plt.plot(epochs_list, adaptive_train_acc, label=\"Adaptive\", linewidth=2, color='brown')\n",
        "plt.plot(epochs_list, k100_train_acc, label=\"k = 1/100\", linewidth=2, color='magenta')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Accuracy (%)\")\n",
        "plt.title(\"Training Accuracy - All Optimizers (MNIST)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mnist_all_train_accuracy.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs_list, sgd_test_errors, label=\"SGD\", linewidth=2, color='blue')\n",
        "plt.plot(epochs_list, k1000_test_errors, label=\"k = 1/1000\", linewidth=2, color='black')\n",
        "plt.plot(epochs_list, gauss_test_errors, label=\"Gaussian\", linewidth=2, color='red')\n",
        "plt.plot(epochs_list, k5_test_errors, label=\"k = 1/5\", linewidth=2, color='green')\n",
        "plt.plot(epochs_list, k10_test_errors, label=\"k = 1/10\", linewidth=2, color='purple')\n",
        "plt.plot(epochs_list, k50_test_errors, label=\"k = 1/50\", linewidth=2, color='orange')\n",
        "plt.plot(epochs_list, adaptive_test_errors, label=\"Adaptive\", linewidth=2, color='brown')\n",
        "plt.plot(epochs_list, k100_test_errors, label=\"k = 1/100\", linewidth=2, color='magenta')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Test Error (Misclassification Ratio)\")\n",
        "plt.title(\"Test Error - All Optimizers (MNIST) [Log Scale]\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mnist_all_test_error.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Done!\\nPlots saved: mnist_all_train_loss.pdf, mnist_all_test_loss.pdf, mnist_all_test_accuracy.pdf, mnist_all_train_accuracy.pdf, mnist_all_test_error.pdf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sharpness_alpha(optimizer):\n",
        "    # Extract values from the optimizer\n",
        "    sharpness_values = optimizer.sharpness_list\n",
        "    alpha_values = optimizer.alpha_list\n",
        "    iterations = list(range(len(sharpness_values)))\n",
        "\n",
        "    # Define figure size and font sizes\n",
        "    figsize = (10, 6)\n",
        "    label_fontsize = 16\n",
        "    title_fontsize = 18\n",
        "    legend_fontsize = 14\n",
        "    tick_fontsize = 14\n",
        "\n",
        "    # Create figure and primary axis\n",
        "    fig, ax1 = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Plot sharpness (left y-axis)\n",
        "    ax1.plot(iterations, sharpness_values, color='blue', label='Sharpness', linewidth=2)\n",
        "    ax1.set_xlabel('Iteration', fontsize=label_fontsize)\n",
        "    ax1.set_ylabel('Sharpness', fontsize=label_fontsize, color='blue')\n",
        "    ax1.tick_params(axis='y', labelcolor='blue', labelsize=tick_fontsize)\n",
        "    ax1.tick_params(axis='x', labelsize=tick_fontsize)\n",
        "\n",
        "    # Create second y-axis for alpha values\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(iterations, alpha_values, color='red', linestyle='--', label='Alpha', linewidth=2)\n",
        "    ax2.set_ylabel('Alpha', fontsize=label_fontsize, color='red')\n",
        "    ax2.tick_params(axis='y', labelcolor='red', labelsize=tick_fontsize)\n",
        "\n",
        "    # Title and legends\n",
        "    plt.title('Sharpness vs Alpha over Iterations', fontsize=title_fontsize)\n",
        "\n",
        "    # Combine legends from both axes\n",
        "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, fontsize=legend_fontsize, loc='upper right')\n",
        "\n",
        "    # Tight layout and save\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"YES_sharpness_vs_alpha.pdf\", bbox_inches=\"tight\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "6ocOQY6_duYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharpness_alpha(optimizer_adaptive)"
      ],
      "metadata": {
        "id": "vSB6CKajd4Zy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}